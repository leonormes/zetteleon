{"assets":[{"title":"HCP Vault Cluster Issuer Configuration","id":"f20fa0cf-e104-4b70-9b76-0e65ff23e90d","type":"CODE","raw":"# hcp-vault-clusterissuer.yaml\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: hcp-vault-issuer\nspec:\n  vault:\n    # Your HCP Vault server address (ensure it's reachable from EKS)\n    server: \"https://your-vault-cluster.private.vault.cloud:8200\"\n    # The path where you enabled the Kubernetes auth method\n    path: \"kubernetes_eks_prod\"\n    # The Kubernetes auth role you created in Terraform\n    role: \"cert-manager\"\n    # The full path to the signing endpoint in the intermediate CA\n    pkiPath: \"pki_int_fitfile/sign/cert-manager-role\"","language":"yaml","time":"3 months ago","created":"2025-08-04T10:49:03.938Z","annotations":[],"updated":"2025-08-11T17:45:43.390Z"},{"title":"Nodepool Upgrade with Image Only","id":"199720ad-81d4-4993-a0cb-8d9013ad02ca","type":"CODE","raw":"az aks nodepool upgrade \\\n--resource-group rg-ff-uks-gp-net \\\n--cluster-name aks-ff-uks-gp-01 \\\n--name system \\\n--node-image-only","language":"sh","time":"3 months ago","created":"2025-08-04T15:27:40.716Z","annotations":[],"updated":"2025-08-04T16:58:18.538Z"},{"title":"Update aks with proxy configuration","id":"dc44aa75-5f96-4184-9d82-94388106e334","type":"CODE","raw":"az aks update \\\n--name aks-ff-uks-gp-01 \\\n--resource-group rg-ff-uks-gp-net \\\n--http-proxy-config ./proxy-config.json","language":"sh","time":"3 months ago","created":"2025-08-04T15:46:50.875Z","annotations":[],"updated":"2025-08-04T15:50:00.230Z"},{"title":"kubectl debug node with image.","id":"dabbe341-3c17-4a46-a5a4-f2bb7ecd76d7","type":"CODE","raw":"kubectl debug node/aks-system-23892849-vmss000002 -it --image=mcr.microsoft.com/cbl-mariner/base/core:2.0\n","language":"sh","time":"3 months ago","created":"2025-08-04T15:47:44.586Z","annotations":[],"updated":"2025-08-13T10:52:29.945Z"},{"title":"Azure Cluster Upgrade","id":"b9128a46-c471-4745-b013-0b004abb2730","type":"CODE","raw":"az aks upgrade \\\n--resource-group rg-ff-uks-gp-net \\\n--name aks-ff-uks-gp-01 \\\n--node-image-only \\\n--yes","language":"sh","time":"3 months ago","created":"2025-08-04T15:57:01.923Z","annotations":[],"updated":"2025-08-13T12:15:12.390Z"},{"title":"kubectl deployment restart with jsonpath","id":"43f572c2-e64a-439e-98fb-de46b13ed3c7","type":"CODE","raw":"kubectl get deployments --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}\\n{end}' | while read ns name; do kubectl rollout restart deployment \"$name\" -n \"$ns\"; done","language":"sh","time":"3 months ago","created":"2025-08-05T08:56:36.852Z","annotations":[],"updated":"2025-08-05T09:19:36.383Z"},{"title":"Rolling Upgrade for Azure Kubernetes Service","id":"dbc836f7-74d2-408c-b10f-f8801d16761b","type":"CODE","raw":"az aks nodepool upgrade \\\n  --resource-group rg-ff-uks-gp-net \\\n  --cluster-name aks-ff-uks-gp-01 \\\n  --name system \\\n  --node-image-only\n\n","language":"sh","time":"3 months ago","created":"2025-08-05T10:20:17.580Z","annotations":[],"updated":"2025-08-12T16:30:38.822Z"},{"title":"EIP Address Validation for NAT Gateway","id":"7930129c-d011-4d58-a055-514342c086bb","type":"CODE","raw":"# Check the NAT Gateway details\naws ec2 describe-nat-gateways --nat-gateway-ids nat-02c1a6d832f6683e5\n\n# Check the EIP details\naws ec2 describe-addresses --allocation-ids eipalloc-0c7a6709d25c111cd","language":"sh","time":"3 months ago","created":"2025-08-14T10:56:28.459Z","annotations":[],"updated":"2025-11-03T13:16:57.175Z"},{"title":"Kubernetes Configuration for Custom Kubernetes System","id":"173940ce-13e9-414e-b893-e30560dc5586","type":"CODE","raw":"apiVersion: v1\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2025-06-25T16:53:13Z\"\n  labels:\n    addonmanager.kubernetes.io/mode: EnsureExists\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n  name: coredns-custom\n  namespace: kube-system\n  resourceVersion: \"395\"\n  uid: 61660d0e-82b9-4bf5-9170-89e14209702c\ndata:\n  onprem-sql.override: |\n    hosts {\n      10.252.169.98 GBCBGPCISQ001.net.addenbrookes.nhs.uk\n      }","language":"yaml","time":"3 months ago","created":"2025-08-05T11:03:57.040Z","annotations":[],"updated":"2025-08-05T14:40:43.411Z"},{"title":"Kubernetes Proxy for Internal Services","id":"685d16b5-b1c4-49d6-9d0b-467b33d32cc0","type":"CODE","raw":"Absolutely! To ensure your Kubernetes pods bypass the proxy for all internal services, you should include **all forms** of each service's DNS name in your `NO_PROXY` (and `no_proxy`) environment variable. This is especially important in Kubernetes, where services may be accessed by short name, namespace-qualified name, or full FQDN.\n\nBelow is a detailed guide and a ready-to-use list for your `cuh-prod-1` namespace, based on your current services.\n\n---\n\n## 1. **Service List and All DNS Forms**\n\nFrom your output:\n```text\nkubectl get svc -n cuh-prod-1\nNAME\ncuh-prod-1-ffcloud-service\ncuh-prod-1-fitconnect-ftc\ncuh-prod-1-frontend-frontend\ncuh-prod-1-minio\ncuh-prod-1-mongodb-b17ef-arbiter-headless\ncuh-prod-1-mongodb-b17ef-headless\ncuh-prod-1-mongodb-b17ef-metrics\ncuh-prod-1-postgresql\ncuh-prod-1-postgresql-hl\nmutating-proxy-webhook\nworkflows-api\n```\n\nFor each service, you should include:\n- The short name (e.g., `cuh-prod-1-ffcloud-service`)\n- The namespace-qualified name (e.g., `cuh-prod-1-ffcloud-service.cuh-prod-1`)\n- The full FQDN (e.g., `cuh-prod-1-ffcloud-service.cuh-prod-1.svc.cluster.local`)\n\n---\n\n## 2. **Recommended NO_PROXY List**\n\nHere‚Äôs a comprehensive list for your `NO_PROXY` variable, including all forms for each service, plus the standard Kubernetes suffixes and localhost:\n\n```env\nNO_PROXY=localhost,127.0.0.1,\\\ncuh-prod-1-ffcloud-service,\\\ncuh-prod-1-ffcloud-service.cuh-prod-1,\\\ncuh-prod-1-ffcloud-service.cuh-prod-1.svc.cluster.local,\\\ncuh-prod-1-fitconnect-ftc,\\\ncuh-prod-1-fitconnect-ftc.cuh-prod-1,\\\ncuh-prod-1-fitconnect-ftc.cuh-prod-1.svc.cluster.local,\\\ncuh-prod-1-frontend-frontend,\\\ncuh-prod-1-frontend-frontend.cuh-prod-1,\\\ncuh-prod-1-frontend-frontend.cuh-prod-1.svc.cluster.local,\\\ncuh-prod-1-minio,\\\ncuh-prod-1-minio.cuh-prod-1,\\\ncuh-prod-1-minio.cuh-prod-1.svc.cluster.local,\\\ncuh-prod-1-mongodb-b17ef-arbiter-headless,\\\ncuh-prod-1-mongodb-b17ef-arbiter-headless.cuh-prod-1,\\\ncuh-prod-1-mongodb-b17ef-arbiter-headless.cuh-prod-1.svc.cluster.local,\\\ncuh-prod-1-mongodb-b17ef-headless,\\\ncuh-prod-1-mongodb-b17ef-headless.cuh-prod-1,\\\ncuh-prod-1-mongodb-b17ef-headless.cuh-prod-1.svc.cluster.local,\\\ncuh-prod-1-mongodb-b17ef-metrics,\\\ncuh-prod-1-mongodb-b17ef-metrics.cuh-prod-1,\\\ncuh-prod-1-mongodb-b17ef-metrics.cuh-prod-1.svc.cluster.local,\\\ncuh-prod-1-postgresql,\\\ncuh-prod-1-postgresql.cuh-prod-1,\\\ncuh-prod-1-postgresql.cuh-prod-1.svc.cluster.local,\\\ncuh-prod-1-postgresql-hl,\\\ncuh-prod-1-postgresql-hl.cuh-prod-1,\\\ncuh-prod-1-postgresql-hl.cuh-prod-1.svc.cluster.local,\\\nmutating-proxy-webhook,\\\nmutating-proxy-webhook.cuh-prod-1,\\\nmutating-proxy-webhook.cuh-prod-1.svc.cluster.local,\\\nworkflows-api,\\\nworkflows-api.cuh-prod-1,\\\nworkflows-api.cuh-prod-1.svc.cluster.local,\\\n.svc,.svc.cluster.local\n```\n\n**Tip:**  \n- The `.svc` and `.svc.cluster.local` suffixes ensure that any Kubernetes service (in any namespace) will bypass the proxy.\n- Always include both uppercase (`NO_PROXY`) and lowercase (`no_proxy`) variants for maximum compatibility.\n\n---\n\n## 3. **How to Update in Your Deployment**\n\nIn your Kubernetes deployment YAML, update the environment section like this:\n\n```yaml\nenv:\n  - name: NO_PROXY\n    value: \"localhost,127.0.0.1,cuh-prod-1-ffcloud-service,cuh-prod-1-ffcloud-service.cuh-prod-1,cuh-prod-1-ffcloud-service.cuh-prod-1.svc.cluster.local,...,.svc,.svc.cluster.local\"\n  - name: no_proxy\n    value: \"localhost,127.0.0.1,cuh-prod-1-ffcloud-service,cuh-prod-1-ffcloud-service.cuh-prod-1,cuh-prod-1-ffcloud-service.cuh-prod-1.svc.cluster.local,...,.svc,.svc.cluster.local\"\n```\n*(Replace `...` with the rest of the entries from the list above.)*\n\n---\n\n## 4. **Why This Matters**\n\n- **Short form**: Used by apps in the same namespace.\n- **Namespace-qualified**: Used by apps in other namespaces.\n- **FQDN**: Used by some libraries and for cross-namespace communication.\n- **Suffixes**: Catch-all for any future services.\n\nIf you only use one form, some requests may still go through the proxy, causing failures (like the 407 Proxy Authentication Required you saw for `spicedb`).\n\n---\n\n## 5. **References**\n\n- [Kubernetes DNS for Services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)\n- [NO_PROXY and FQDNs in Kubernetes](https://github.com/kubernetes/kubernetes/issues/32286)\n\n---\n\n**Let me know if you want a script to generate this list automatically, or if you need help patching your deployment YAMLs!**","language":"md","time":"3 months ago","created":"2025-08-04T09:27:19.833Z","annotations":[],"updated":"2025-11-08T12:15:18.811Z"},{"title":"Fetch Vulnerability Reports from Kubernetes","id":"3d94f585-01a3-44d4-9304-c05d275a16b8","type":"CODE","raw":"import json\nimport subprocess\nfrom datetime import datetime\nimport requests\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ntop_vulns_count = 5\nmin_score = 6\n\ndef fetch_description(vuln_id):\n    nvd_url = f\"https://access.redhat.com/hydra/rest/securitydata/cve/{vuln_id}\"\n    try:\n        response = requests.get(nvd_url, timeout=10)\n        data = response.json()\n        # description = data.get(\"bugzilla\", {}).get(\"description\")\n        description = data.get(\"details\", {})\n\n        if description:\n            return description\n    except Exception as e:\n        return f\"Description not available (error: {e})\"\n    return \"Description not available\"\n\nprint(\"üîç Retrieving vulnerability reports from Kubernetes...\")\nresult = subprocess.run(\n    [\"kubectl\", \"get\", \"vulnerabilityreports\", \"-A\", \"-o\", \"json\"],\n    capture_output=True, text=True\n)\n\nvuln_reports = json.loads(result.stdout)\nseverity_groups = {\"CRITICAL\": [], \"HIGH\": [], \"MEDIUM\": [], \"LOW\": []}\nvuln_tasks = []\n\nprint(\"üìä Processing vulnerabilities and preparing description fetch tasks...\")\nfor item in tqdm(vuln_reports.get(\"items\", []), desc=\"Processing reports\"):\n    namespace = item[\"metadata\"][\"namespace\"]\n    name = item[\"metadata\"][\"name\"]\n    vulnerabilities = item.get(\"report\", {}).get(\"vulnerabilities\", [])\n\nfor vuln in vulnerabilities:\n    vuln_id = vuln.get(\"vulnerabilityID\")\n    severity = vuln.get(\"severity\")\n    score = vuln.get(\"score\")\n    if severity in severity_groups and vuln_id not in [v[\"vulnerability_id\"] for v in severity_groups[severity]] and score is not None and score >= min_score:\n        pkg_name = vuln.get(\"pkgName\")\n        vuln_data = {\n            \"namespace\": namespace,\n            \"report_name\": name,\n            \"vulnerability_id\": vuln_id,\n            \"resource\": vuln.get(\"resource\"),\n            \"severity\": severity,\n            \"score\": score,\n            \"title\": vuln.get(\"title\"),\n            \"redhat_url\": f\"https://access.redhat.com/security/cve/{vuln_id}\",\n            \"nvd_url\": f\"https://nvd.nist.gov/vuln/detail/{vuln_id}\"\n        }\n        severity_groups[severity].append(vuln_data)\n        vuln_tasks.append((vuln_id, vuln_data))\n\nprint(\"‚ö° Fetching vulnerability descriptions concurrently...\")\n# with ThreadPoolExecutor(max_workers=16) as executor:\n#     future_to_vuln = {executor.submit(fetch_description, vuln_id): vuln_data for vuln_id, vuln_data in vuln_tasks}\n#     for future in tqdm(as_completed(future_to_vuln), total=len(future_to_vuln), desc=\"Fetching descriptions\"):\n#         vuln_data = future_to_vuln[future]\n#         try:\n#             description = future.result()\n#         except Exception as e:\n#             description = f\"Description not available (error: {e})\"\n#         vuln_data[\"description\"] = description\n\nprint(f\"üìå Selecting top {top_vulns_count} vulnerabilities for each severity level, where the miniumum score is {min_score}...\")\ntop_vulnerabilities = {}\nfor severity, vulns in severity_groups.items():\n    sorted_vulns = sorted(vulns, key=lambda x: x[\"score\"] if x[\"score\"] is not None else 0, reverse=True)\n    top_vulnerabilities[severity] = sorted_vulns[:top_vulns_count]\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nfilename = f\"top_vulnerabilities_all_severities_{timestamp}.json\"\n\nwith open(filename, \"w\") as f:\n    json.dump(top_vulnerabilities, f, indent=2)\n\nprint(f\"‚úÖ Report saved to '{filename}'\")","language":"py","time":"2 months ago","created":"2025-08-28T10:10:13.226Z","annotations":[],"updated":"2025-11-08T12:10:54.450Z"},{"title":"Restart CoreDNS and Verify with Debug Pod","id":"07746667-82f6-454b-a75a-96f8e5672a79","type":"CODE","raw":"# Restart CoreDNS\nkubectl rollout restart deployment coredns -n kube-system\n\n# Wait for the restart to complete\nkubectl rollout status deployment/coredns -n kube-system\n\n# Verify with a debug pod\nkubectl run -it --rm --image=nicolaka/netshoot dns-test -- /bin/bash -c \"dig GBCBGPCISQ001.net.addenbrookes.nhs.uk\"","language":"sh","time":"3 months ago","created":"2025-08-05T11:10:45.401Z","annotations":[],"updated":"2025-11-08T12:11:16.362Z"},{"title":"Variable Variables for Root CA PKI Engine","id":"e62c360a-be51-4e9f-9425-9e56c8dee772","type":"CODE","raw":"# -----------------------------------------------------------------------------\n# VARIABLES\n# -----------------------------------------------------------------------------\nvariable \"organization\" {\n  description = \"Organisation name for certificates\"\n  type        = string\n  default     = \"FitFile Inc\"\n}\n\nvariable \"pki_root_path\" {\n  description = \"Path for the Root PKI Secrets Engine\"\n  type        = string\n  default     = \"pki_root_fitfile\"\n}\n\nvariable \"pki_int_path\" {\n  description = \"Path for the Intermediate PKI Secrets Engine\"\n  type        = string\n  default     = \"pki_int_fitfile\"\n}\n\nvariable \"kubernetes_auth_path\" {\n  description = \"Path for the Kubernetes authentication backend\"\n  type        = string\n  default     = \"kubernetes_eks_prod\"\n}\n\nvariable \"allowed_domains\" {\n  description = \"List of allowed domains for certificate issuance\"\n  type        = list(string)\n  default     = [\"fitfile.net\"]\n}\n\nvariable \"kubernetes_host\" {\n  description = \"Kubernetes API URL for the EKS cluster\"\n  type        = string\n}\n\nvariable \"kubernetes_ca_cert\" {\n  description = \"PEM-encoded CA certificate for the Kubernetes API\"\n  type        = string\n  sensitive   = true\n}\n\nvariable \"token_reviewer_jwt\" {\n  description = \"A long-lived Kubernetes service account token that has permission to perform TokenReview\"\n  type        = string\n  sensitive   = true\n}\n\n# -----------------------------------------------------------------------------\n# ROOT CA SETUP\n# -----------------------------------------------------------------------------\nresource \"vault_mount\" \"pki_root\" {\n  path        = var.pki_root_path\n  type        = \"pki\"\n  description = \"FitFile Root CA PKI Engine\"\n  # 20 year max TTL for the root\n  max_lease_ttl_seconds = 630720000\n}\n\nresource \"vault_pki_secret_backend_root_cert\" \"root\" {\n  depends_on = [vault_mount.pki_root]\n\n  backend     = vault_mount.pki_root.path\n  type        = \"internal\"\n  common_name = \"FitFile Root CA\"\n  # 10 year TTL for the root certificate\n  ttl         = \"87600h\"\n  key_type    = \"rsa\"\n  key_bits    = 4096\n  organization = var.organization\n}\n\n# -----------------------------------------------------------------------------\n# INTERMEDIATE CA SETUP\n# -----------------------------------------------------------------------------\nresource \"vault_mount\" \"pki_int\" {\n  path        = var.pki_int_path\n  type        = \"pki\"\n  description = \"FitFile Intermediate CA PKI Engine\"\n  # 5 year max TTL for the intermediate\n  max_lease_ttl_seconds = 157680000\n}\n\nresource \"vault_pki_secret_backend_intermediate_cert_request\" \"intermediate\" {\n  depends_on = [vault_mount.pki_int]\n\n  backend     = vault_mount.pki_int.path\n  type        = \"internal\"\n  common_name = \"FitFile Intermediate CA\"\n}\n\nresource \"vault_pki_secret_backend_root_sign_intermediate\" \"intermediate\" {\n  depends_on = [vault_pki_secret_backend_root_cert.root]\n\n  backend     = vault_mount.pki_root.path\n  csr         = vault_pki_secret_backend_intermediate_cert_request.intermediate.csr\n  common_name = \"FitFile Intermediate CA\"\n  # 3 year TTL for the intermediate certificate\n  ttl         = \"26280h\"\n  format      = \"pem_bundle\"\n}\n\nresource \"vault_pki_secret_backend_intermediate_set_signed\" \"intermediate\" {\n  backend     = vault_mount.pki_int.path\n  certificate = vault_pki_secret_backend_root_sign_intermediate.intermediate.certificate\n}\n\n# -----------------------------------------------------------------------------\n# CERT-MANAGER ROLE & POLICY\n# -----------------------------------------------------------------------------\nresource \"vault_pki_secret_backend_role\" \"cert_manager_role\" {\n  depends_on = [vault_pki_secret_backend_intermediate_set_signed.intermediate]\n\n  backend          = vault_mount.pki_int.path\n  name             = \"cert-manager-role\"\n  allowed_domains  = var.allowed_domains\n  allow_subdomains = true\n  # 30 day TTL for leaf certificates\n  max_ttl          = \"720h\"\n}\n\nresource \"vault_policy\" \"cert_manager_policy\" {\n  name = \"cert-manager-policy\"\n\n  policy = <<EOT\n# Allow cert-manager to sign certificates using its role\npath \"${vault_mount.pki_int.path}/sign/${vault_pki_secret_backend_role.cert_manager_role.name}\" {\n  capabilities = [\"create\", \"update\"]\n}\nEOT\n}\n\n# -----------------------------------------------------------------------------\n# KUBERNETES AUTHENTICATION\n# -----------------------------------------------------------------------------\nresource \"vault_auth_backend\" \"kubernetes\" {\n  type = \"kubernetes\"\n  path = var.kubernetes_auth_path\n}\n\nresource \"vault_kubernetes_auth_backend_config\" \"config\" {\n  backend            = vault_auth_backend.kubernetes.path\n  kubernetes_host    = var.kubernetes_host\n  kubernetes_ca_cert = var.kubernetes_ca_cert\n  token_reviewer_jwt = var.token_reviewer_jwt\n}\n\nresource \"vault_kubernetes_auth_backend_role\" \"cert_manager_auth\" {\n  backend                          = vault_auth_backend.kubernetes.path\n  role_name                        = \"cert-manager\"\n  bound_service_account_names      = [\"cert-manager\"]\n  bound_service_account_namespaces = [\"cert-manager\"]\n  token_policies                   = [vault_policy.cert_manager_policy.name]\n  token_ttl                        = 3600 # 1 hour\n}","language":"tf","time":"3 months ago","created":"2025-08-04T10:48:48.283Z","annotations":[],"updated":"2025-11-01T14:58:40.449Z"},{"title":"Discover Images in Chart","id":"98defb65-bddc-46c8-b87c-28ffc334890a","type":"CODE","raw":"discover_images_in_chart() {\n    local chart_dir=\"$1\"\n\n    if [ ! -d \"$chart_dir\" ]; then\n        echo \"Error: Chart directory '$chart_dir' not found.\" >&2\n        return 1\n    fi\n\n    # Method 1: Use helm template to render the chart and extract image references\n    local rendered_images=\"\"\n    if command -v helm &> /dev/null; then\n        rendered_images=$(helm template \"$chart_dir\" 2>/dev/null | \\\n            grep -E 'image:' | \\\n            sed -e 's/^[[:space:]]*image:[[:space:]]*//' -e 's/\"//g' -e \"s/'//g\" | \\\n            grep -E '^[a-zA-Z0-9./_-]+:[a-zA-Z0-9._-]+$' | \\\n            sort -u\n        )\n    fi\n\n    # Method 2: Extract from values.yaml for simple repository and tag combinations.\n    local values_images=\"\"\n    if [ -f \"$chart_dir/values.yaml\" ]; then\n        values_images=$(yq eval '.. | select(has(\"repository\") and has(\"tag\")) | .repository + \":\" + .tag' \"$chart_dir/values.yaml\" 2>/dev/null)\n    fi\n\n    # Combine, sort, and unique the results from all methods\n    (echo \"$rendered_images\"; echo \"$values_images\") | \\\n        grep -v -e '^$' -e 'null' | \\\n        sort -u\n}\n","language":"sh","time":"2 months ago","created":"2025-09-05T15:55:06.393Z","annotations":[],"updated":"2025-11-08T12:13:26.260Z"},{"title":"FITFILE Organization and Domain Configuration","id":"07cccc4d-ea81-4dee-93ae-bdb19787bce6","type":"CODE","raw":"organization      = \"FITFILE\"\nroot_common_name  = \"FITFILE Root CA\"\nint_common_name   = \"EOE SDE Intermediate CA\"\ndomain            = \"fitfile.co.uk\"\nallowed_domains   = [\"fitfile.co.uk\", \"*.fitfile.co.uk\"]\ncert_country      = \"GB\"\ncert_province     = \"London\"\ncert_locality     = \"London\"\ncert_ttl          = 720 # 30 days","language":"tf","time":"3 months ago","created":"2025-08-05T13:23:45.743Z","annotations":[],"updated":"2025-08-06T08:52:59.035Z"},{"title":"Deployment Generator with Configurations and Output","id":"cff71cac-4e19-4bc8-b43a-e7ef16684ddc","type":"CODE","raw":"#!/bin/bash\n# tools/deployment-generator/generate.sh\n\nDEPLOYMENT_NAME=$1\n\n# Read enabled components\nENABLED_COMPONENTS=$(yq '.[]' deployments/$DEPLOYMENT_NAME/enabled-components.yaml)\n\n# Generate ffnode values.yaml\necho \"# Generated deployment configuration for $DEPLOYMENT_NAME\" > /tmp/generated-values.yaml\n\n# Merge component configurations\nfor component in $ENABLED_COMPONENTS; do\n    # Merge default values\n    yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' \\\n        /tmp/generated-values.yaml \\\n        components/$component/default-values.yaml > /tmp/merged.yaml\n    \n    # Apply deployment-specific overrides if they exist\n    if [[ -f \"deployments/$DEPLOYMENT_NAME/component-overrides/$component.yaml\" ]]; then\n        yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' \\\n            /tmp/merged.yaml \\\n            deployments/$DEPLOYMENT_NAME/component-overrides/$component.yaml > /tmp/final.yaml\n        mv /tmp/final.yaml /tmp/merged.yaml\n    fi\n    \n    mv /tmp/merged.yaml /tmp/generated-values.yaml\ndone\n\n# Output final configuration\nmv /tmp/generated-values.yaml deployments/$DEPLOYMENT_NAME/generated-values.yaml\necho \"Generated deployment configuration: deployments/$DEPLOYMENT_NAME/generated-values.yaml\"\n","language":"sh","time":"2 months ago","created":"2025-09-07T15:41:05.921Z","annotations":[],"updated":"2025-11-01T14:50:52.845Z"},{"title":"FitFile Root CA","id":"4a3e7a30-7ae0-45f5-8156-64bc24b2aff2","type":"CODE","raw":"üëë FitFile Root CA (in HCP Vault, highly secured)\n       ‚îÇ\n       ‚îú‚îÄ‚îÄ signs --> üè¢ SDE Intermediate CA\n       ‚îÇ             ‚îÇ\n       ‚îÇ             ‚îî‚îÄ‚îÄ issues certs for --> *.sde.fitfile.net\n       ‚îÇ\n       ‚îú‚îÄ‚îÄ signs --> üè¢ CUH Intermediate CA\n       ‚îÇ             ‚îÇ\n       ‚îÇ             ‚îî‚îÄ‚îÄ issues certs for --> *.cuh.fitfile.net\n       ‚îÇ\n       ‚îî‚îÄ‚îÄ signs --> üè¢ New Customer Intermediate CA\n                     ‚îÇ\n                     ‚îî‚îÄ‚îÄ issues certs for --> *.newcustomer.fitfile.net","language":"md","time":"3 months ago","created":"2025-08-06T14:08:48.776Z","annotations":[],"updated":"2025-08-06T14:08:51.799Z"},{"title":"FitFile Repository List and Show Tags","id":"eac7d0ae-3bf3-4b7e-9537-932797d19499","type":"CODE","raw":"az acr repository list --name fitfileregistry --output json | jq -r '.[]' | while read repo; do\n  echo \"Repository: $repo\"\n  az acr repository show-tags --name fitfileregistry --repository \"$repo\" --output table\n  echo \"\"\ndone\n","language":"sh","time":"2 months ago","created":"2025-09-09T12:38:10.368Z","annotations":[],"updated":"2025-11-08T12:10:23.129Z"},{"title":"Show Private DNS Zone","id":"c4bb8ac3-3128-4ddf-a743-5a56346e4a96","type":"CODE","raw":"az network private-dns zone show --resource-group rg-ff-uks-gp-net --name privatelink.fitfile.net --query \"properties.privateZoneNameConfiguration.privateDnsZoneConfigs[0].privateIpAddress\" -o tsv","language":"sh","time":"3 months ago","created":"2025-08-12T10:51:14.780Z","annotations":[],"updated":"2025-08-12T10:51:16.782Z"},{"title":"EKS Node Egress Troubleshooting Script","id":"99616a61-aab2-4a37-8082-deb3cdda28ad","type":"CODE","raw":"#!/bin/bash\n\n# EKS Node Egress Troubleshooting Script\n# This script diagnoses outbound connectivity issues from an EKS worker node,\n# which is the root cause of ImagePullBackOff errors when nodes cannot reach\n# an external container registry.\n\nset -eo pipefail\n\n# --- Configuration ---\n# Details from the problem description and logs\nCLUSTER_NAME=\"hie-prod-34\"\nREGION=\"eu-west-2\"\nAPP_LABEL_SELECTOR=\"app.kubernetes.io/name=relay\"\nRELAY_NAMESPACE=\"hutch\"\nHTTPS_PORT=\"443\"\n\n# --- Script Colours ---\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nCYAN='\\033[0;36m'\nNC='\\033[0m' # No Colour\n\necho -e \"${GREEN}üîç EKS Node Egress Troubleshooting for Cluster '${CLUSTER_NAME}'${NC}\"\necho \"==========================================================\"\n\n# Function to check if a command exists\ncommand_exists() {\n    command -v \"$1\" >/dev/null 2>&1\n}\n\n# --- Prerequisite Checks ---\ncheck_prerequisites() {\n    echo \"üîç Checking for required tools (aws, kubectl, jq)...\"\n    if ! command_exists aws; then\n        echo -e \"${RED}‚ùå AWS CLI is not installed. Please install it to continue.${NC}\"\n        exit 1\n    fi\n    if ! command_exists kubectl; then\n        echo -e \"${RED}‚ùå kubectl is not installed. Please install it to continue.${NC}\"\n        exit 1\n    fi\n    if ! command_exists jq; then\n        echo -e \"${RED}‚ùå jq is not installed. Please install it to continue.${NC}\"\n        exit 1\n    fi\n\n    if ! aws sts get-caller-identity >/dev/null 2>&1; then\n        echo -e \"${RED}‚ùå AWS CLI not configured or credentials invalid. Run 'aws configure'.${NC}\"\n        exit 1\n    fi\n    echo -e \"${GREEN}‚úÖ AWS CLI is configured.${NC}\"\n\n    if ! kubectl cluster-info >/dev/null 2>&1; then\n        echo -e \"${YELLOW}‚ö†Ô∏è  kubectl not configured. Attempting to configure...${NC}\"\n        aws eks update-kubeconfig --region ${REGION} --name ${CLUSTER_NAME}\n        if ! kubectl cluster-info >/dev/null 2>&1; then\n            echo -e \"${RED}‚ùå Failed to configure kubectl for cluster ${CLUSTER_NAME}.${NC}\"\n            exit 1\n        fi\n    fi\n    echo -e \"${GREEN}‚úÖ kubectl is configured for cluster '${CLUSTER_NAME}'.${NC}\"\n}\n\n# --- Main Diagnostic Function ---\nrun_diagnostics() {\n    # --- Pod and Node Identification ---\n    echo -e \"\\n--- Identifying Failing Pod and Node ---\"\n    POD_JSON=$(kubectl get pods -n \"${RELAY_NAMESPACE}\" -l \"${APP_LABEL_SELECTOR}\" -o json)\n    \n    if [[ $(echo \"$POD_JSON\" | jq '.items | length') -eq 0 ]]; then\n        echo -e \"${RED}‚ùå No pods found with label '${APP_LABEL_SELECTOR}' in namespace '${RELAY_NAMESPACE}'. Aborting.${NC}\"\n        return 1\n    fi\n\n    POD_NAME=$(echo \"$POD_JSON\" | jq -r '.items[0].metadata.name')\n    NODE_NAME=$(echo \"$POD_JSON\" | jq -r '.items[0].spec.nodeName')\n    \n    echo \"‚úÖ Found Pod: ${POD_NAME}\"\n    echo \"‚úÖ Pod is scheduled on Node: ${NODE_NAME}\"\n\n    # --- EC2 Instance and Network Details ---\n    echo -e \"\\n--- Fetching EC2 and VPC Details ---\"\n    INSTANCE_ID=$(aws ec2 describe-instances --region \"${REGION}\" --filters \"Name=private-dns-name,Values=${NODE_NAME}\" --query \"Reservations[0].Instances[0].InstanceId\" --output text)\n    if [[ -z \"$INSTANCE_ID\" || \"$INSTANCE_ID\" == \"None\" ]]; then\n        echo -e \"${RED}‚ùå Could not find EC2 instance for node '${NODE_NAME}'.${NC}\"\n        return 1\n    fi\n    \n    INSTANCE_DETAILS=$(aws ec2 describe-instances --instance-ids \"${INSTANCE_ID}\" --region \"${REGION}\" --query \"Reservations[0].Instances[0]\")\n    SUBNET_ID=$(echo \"$INSTANCE_DETAILS\" | jq -r '.SubnetId')\n    VPC_ID=$(echo \"$INSTANCE_DETAILS\" | jq -r '.VpcId')\n    SG_ID=$(echo \"$INSTANCE_DETAILS\" | jq -r '.SecurityGroups[0].GroupId')\n\n    echo \"‚úÖ Instance ID: ${INSTANCE_ID}\"\n    echo \"‚úÖ VPC ID: ${VPC_ID}\"\n    echo \"‚úÖ Subnet ID: ${SUBNET_ID}\"\n    echo \"‚úÖ Security Group ID: ${SG_ID}\"\n\n    # --- Route Table Analysis ---\n    echo -e \"\\n--- Analysing Route Table for Subnet ${SUBNET_ID} ---\"\n    ROUTE_TABLE_ID=$(aws ec2 describe-route-tables --region \"${REGION}\" --filters \"Name=association.subnet-id,Values=${SUBNET_ID}\" --query \"RouteTables[0].RouteTableId\" --output text)\n    echo \"üîç Subnet is associated with Route Table: ${ROUTE_TABLE_ID}\"\n    aws ec2 describe-route-tables --route-table-ids \"${ROUTE_TABLE_ID}\" --region \"${REGION}\" | jq -r '.RouteTables[0].Routes[] | \"  - Dest: \\(.DestinationCidrBlock // .DestinationPrefixListId // \"N/A\"), Target: \\(.GatewayId // .NatGatewayId // .TransitGatewayId // \"N/A\"), State: \\(.State)\"'\n\n    DEFAULT_ROUTE_TARGET=$(aws ec2 describe-route-tables --route-table-ids \"${ROUTE_TABLE_ID}\" --region \"${REGION}\" --query \"RouteTables[0].Routes[?DestinationCidrBlock=='0.0.0.0/0'] | [0]\")\n    \n    if [[ -z \"$DEFAULT_ROUTE_TARGET\" || \"$DEFAULT_ROUTE_TARGET\" == \"null\" ]]; then\n        echo -e \"${RED}‚ùå [FAIL] No default route (0.0.0.0/0) found in the route table. This is the primary cause of no internet access.${NC}\"\n        ROUTE_OK=1\n    elif [[ $(echo \"$DEFAULT_ROUTE_TARGET\" | jq -r '.NatGatewayId') != \"null\" ]]; then\n        echo -e \"${GREEN}‚úÖ [PASS] Default route points to a NAT Gateway.\"\n    elif [[ $(echo \"$DEFAULT_ROUTE_TARGET\" | jq -r '.GatewayId') != \"null\" ]]; then\n        echo -e \"${GREEN}‚úÖ [PASS] Default route points to an Internet Gateway.\"\n    else\n        echo -e \"${RED}‚ùå [FAIL] Default route (0.0.0.0/0) does not point to a NAT Gateway or an Internet Gateway.${NC}\"\n        ROUTE_OK=1\n    fi\n\n    # --- Security Group Egress Analysis ---\n    echo -e \"\\n--- Analysing OUTBOUND Rules for Security Group ${SG_ID} ---\"\n    aws ec2 describe-security-groups --group-ids \"${SG_ID}\" --region \"${REGION}\" | jq -r '.SecurityGroups[0].IpPermissionsEgress[] | \"  - Protocol: \\(.IpProtocol), Port: \\(.FromPort // \"All\")-\\(.ToPort // \"All\"), Dest: \\(.IpRanges[0].CidrIp // .PrefixListIds[0].PrefixListId // \"N/A\")\"'\n    \n    HTTPS_EGRESS_EXISTS=$(aws ec2 describe-security-groups --group-ids \"${SG_ID}\" --region \"${REGION}\" --query \"SecurityGroups[0].IpPermissionsEgress[?IpProtocol=='tcp' && FromPort<=\\`${HTTPS_PORT}\\` && ToPort>=\\`${HTTPS_PORT}\\` && IpRanges[?CidrIp=='0.0.0.0/0']] | length(@)\")\n    if [[ \"$HTTPS_EGRESS_EXISTS\" -eq 0 ]]; then\n        echo -e \"${RED}‚ùå [FAIL] No outbound rule found allowing TCP traffic on port ${HTTPS_PORT} to 0.0.0.0/0.${NC}\"\n        SG_OK=1\n    else\n        echo -e \"${GREEN}‚úÖ [PASS] Security group allows outbound HTTPS traffic to the internet.${NC}\"\n    fi\n\n    # --- Network ACL Analysis ---\n    echo -e \"\\n--- Analysing Network ACL for Subnet ${SUBNET_ID} ---\"\n    NACL_ID=$(aws ec2 describe-network-acls --filters \"Name=association.subnet-id,Values=${SUBNET_ID}\" --query 'NetworkAcls[0].NetworkAclId' --output text)\n    echo \"üîç Subnet is associated with NACL: ${NACL_ID}\"\n    echo -e \"${CYAN}  Outbound Rules:${NC}\"\n    aws ec2 describe-network-acls --network-acl-id \"${NACL_ID}\" --region \"${REGION}\" | jq -r '.NetworkAcls[0].Entries[] | select(.Egress == true) | \"    - Rule \\(.RuleNumber): \\(.RuleAction) \\(.Protocol) Port \\(.PortRange.From // \"All\")-\\(.PortRange.To // \"All\") Dest \\(.CidrBlock)\"'\n    echo -e \"${CYAN}  Inbound Rules (for return traffic):${NC}\"\n    aws ec2 describe-network-acls --network-acl-id \"${NACL_ID}\" --region \"${REGION}\" | jq -r '.NetworkAcls[0].Entries[] | select(.Egress == false) | \"    - Rule \\(.RuleNumber): \\(.RuleAction) \\(.Protocol) Port \\(.PortRange.From // \"All\")-\\(.PortRange.To // \"All\") Source \\(.CidrBlock)\"'\n    \n    # Check for specific deny rules that could block the connection\n    NACL_DENY_EGRESS=$(aws ec2 describe-network-acls --network-acl-ids \"${NACL_ID}\" --query \"NetworkAcls[0].Entries[?RuleAction=='deny' && PortRange.From <= \\`${HTTPS_PORT}\\` && PortRange.To >= \\`${HTTPS_PORT}\\` && Egress==\\`true\\`] | length(@)\")\n    NACL_DENY_INGRESS_EPHEMERAL=$(aws ec2 describe-network-acls --network-acl-ids \"${NACL_ID}\" --query \"NetworkAcls[0].Entries[?RuleAction=='deny' && PortRange.From <= \\`1024\\` && PortRange.To >= \\`65535\\` && Egress==\\`false\\`] | length(@)\")\n\n    if [[ \"$NACL_DENY_EGRESS\" -gt 0 ]]; then\n        echo -e \"${RED}‚ùå [FAIL] NACL (${NACL_ID}) has a rule DENYING OUTBOUND traffic on port ${HTTPS_PORT}.${NC}\"\n        NACL_OK=1\n    elif [[ \"$NACL_DENY_INGRESS_EPHEMERAL\" -gt 0 ]]; then\n        echo -e \"${RED}‚ùå [FAIL] NACL (${NACL_ID}) has a rule DENYING INBOUND traffic on the ephemeral ports (1024-65535), which could block return traffic.${NC}\"\n        NACL_OK=1\n    else\n        echo -e \"${GREEN}‚úÖ [PASS] NACL rules appear to allow the required outbound and return traffic.${NC}\"\n    fi\n}\n\n# --- Main Execution ---\nmain() {\n    check_prerequisites\n    ROUTE_OK=0; SG_OK=0; NACL_OK=0\n    run_diagnostics\n\n    echo -e \"\\n--- üìã Final Diagnosis & Recommendations ---\"\n    if [[ \"$ROUTE_OK\" -ne 0 ]]; then\n        echo -e \"${YELLOW}Primary Issue: The route table for the EKS worker subnet is missing a default route ('0.0.0.0/0') to an Internet Gateway or NAT Gateway.${NC}\"\n        echo \"Solution: Add a default route to the route table '${ROUTE_TABLE_ID}' that points to your VPC's NAT Gateway or Internet Gateway.\"\n    elif [[ \"$SG_OK\" -ne 0 ]]; then\n        echo -e \"${YELLOW}Primary Issue: The worker node's security group '${SG_ID}' is blocking outbound HTTPS traffic.${NC}\"\n        echo \"Solution: Add an outbound rule to '${SG_ID}' that allows TCP traffic on port 443 to destination 0.0.0.0/0.\"\n    elif [[ \"$NACL_OK\" -ne 0 ]]; then\n        echo -e \"${YELLOW}Primary Issue: The Network ACL '${NACL_ID}' is blocking the required traffic.${NC}\"\n        echo \"Solution: Review the outbound rules to ensure port 443 is allowed, and review the inbound rules to ensure the ephemeral return ports (1024-65535) are allowed.\"\n    else\n        echo -e \"${GREEN}‚úÖ All networking checks passed. The egress path from the node appears to be correctly configured. If image pulls are still failing, consider:\"\n        echo \"  1. DNS Resolution Issues within the VPC.\"\n        echo \"  2. Authentication issues with the Azure Container Registry (check 'imagePullSecrets').\"\n        echo \"  3. Temporary issues with the Azure Container Registry itself.\"\n    fi\n}\n\nmain \"$@\"\n","language":"sh","time":"about a month ago","created":"2025-09-25T12:02:47.435Z","annotations":[],"updated":"2025-09-26T09:32:27.784Z"},{"title":"Set Private DNS records for IPv6","id":"23232423-3428-4c4b-9bcd-1cd0a57e4c1b","type":"CODE","raw":"az network private-dns record-set a list -g rg-ff-uks-gp-net -z privatelink.fitfile.net\naz network private-dns record-set aaaa list -g rg-ff-uks-gp-net -z privatelink.fitfile.net # For IPv6 records","language":"sh","time":"3 months ago","created":"2025-08-12T11:40:30.491Z","annotations":[],"updated":"2025-08-12T11:40:32.496Z"},{"title":"Allow Actions for ECR Access","id":"60931fdf-018e-4934-920f-fa03ccf8d42c","type":"CODE","raw":"{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"eks:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"elasticloadbalancing:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ssm:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"route53:*\",\n                \"route53domains:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"acm:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kms:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"network-firewall:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr-public:GetAuthorizationToken\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}","language":"json","time":"about a month ago","created":"2025-10-09T11:46:45.696Z","annotations":[],"updated":"2025-11-01T15:01:32.063Z"},{"title":"VPC Subnets and Route Tables for Internet Gateway","id":"910bfd79-05b6-4b4a-bb2c-c0087abb6489","type":"CODE","raw":"# 1. List all subnets in your VPC\n# Replace VPC_ID with your VPC ID\naws ec2 describe-subnets --filters \"Name=vpc-id,Values=VPC_ID\" \\\n  --query 'Subnets[].[SubnetId, CidrBlock, MapPublicIpOnLaunch, Tags[?Key==`Name`].Value | [0], AvailabilityZone]' \\\n  --output table\n\n# 2. Check route tables for public subnets (look for routes to Internet Gateway)\naws ec2 describe-route-tables \\\n  --filters \"Name=vpc-id,Values=VPC_ID\" \\\n  --query 'RouteTables[].[Associations[].SubnetId, Routes[?GatewayId!=null && starts_with(GatewayId, 'igw-')]]' \\\n  --output table\n\n# 3. Check for existing NAT Gateways\naws ec2 describe-nat-gateways \\\n  --filter \"Name=vpc-id,Values=VPC_ID\" \\\n  --query 'NatGateways[?State==`available`].[NatGatewayId, SubnetId, NatGatewayAddresses[0].PublicIp]' \\\n  --output table\n\n# 4. Check for existing Internet Gateways\naws ec2 describe-internet-gateways \\\n  --filters \"Name=attachment.vpc-id,Values=VPC_ID\" \\\n  --query 'InternetGateways[*].InternetGatewayId' \\\n  --output text","language":"sh","time":"3 months ago","created":"2025-08-13T09:08:23.279Z","annotations":[],"updated":"2025-11-01T15:01:46.447Z"},{"title":"Regex for Regular Patterns","id":"1b9f4a08-c8b2-409e-bd30-51add857e7f8","type":"CODE","raw":":%s/ \\[[0-9,\\- ]\\+\\]//g\n","language":"sh","time":"24 days ago","created":"2025-10-16T14:04:45.885Z","annotations":[],"updated":"2025-10-25T11:11:48.808Z"},{"title":"Kubectl run with image pull secret.","id":"34b78626-d7ee-4c8f-847b-44a6625e0294","type":"CODE","raw":"kubectl run -it --rm --image=fitfileregistry.azurecr.io/busybox:latest dns-test3 -n cuh-prod-1 \\\n--overrides='{ \"spec\": { \"template\": { \"spec\": { \"imagePullSecrets\": [{\"name\": \"fitfile-image-pull-secret\"}] } } } }' \\\n-- /bin/sh","language":"sh","time":"3 months ago","created":"2025-08-13T11:11:58.165Z","annotations":[],"updated":"2025-08-13T13:49:10.309Z"},{"title":"Environment and Project Structure for S3 Fitfile","id":"ddaef7c6-9191-4dd5-9214-afc2fb772130","type":"CODE","raw":"For each environment this is different, additionally for each project it is different\n\n- dlz-tst-s3-fitfile-ff2504-339713007003\n\nStructure `dlz-{env}-s3-fitfile-{sde_project}-{account_id}`. ","language":"md","time":"17 days ago","created":"2025-10-23T09:33:02.839Z","annotations":[],"updated":"2025-11-03T13:15:32.074Z"},{"title":"GitHub Container Registry Authenticate and Pull Private Image","id":"6228d52e-5435-441a-ab91-b44ab618cd6f","type":"CODE","raw":"# Authenticate to GitHub Container Registry (ghcr.io)\n# Commands to Authenticate Docker to ghcr.io\n# Refresh GitHub CLI Token with read:packages Scope\n\ngh auth refresh -h github.com -s read:packages\n\n# Use the Token to Authenticate Docker\n\ngh auth token | docker login ghcr.io -u leonormes --password-stdin\n\n# Example: Pull a Private Image\n\ndocker pull ghcr.io/thehyve/fitfile_etl_container_mkuh:0.4.3-test\naz acr login --name fitfileregistry\ndocker tag ghcr.io/thehyve/fitfile_etl_container_mkuh:0.4.4-test fitfileregistry.azurecr.io/thehyve/fitfile_etl_container_mkuh:0.4.4-test\ndocker push fitfileregistry.azurecr.io/thehyve/fitfile_etl_container_mkuh:0.4.4-test\n","language":"sh","time":"16 days ago","created":"2025-10-24T09:33:47.255Z","annotations":[],"updated":"2025-11-08T12:12:55.601Z"},{"title":"Environment Variables for Vault Secrets","id":"e8ca6fd6-ee45-462e-820b-3cfe46993961","type":"CODE","raw":"extraEnvVars:\n  - name: NHS_SITE\n    value: mkuh\n  # Existing Vault secrets\n  - name: QCR_BUCKET\n    valueFrom:\n      secretKeyRef:\n        name: thehyve\n        key: qcr_bucket\n  # Add your new secret reference:\n  - name: YOUR_SECRET_VAR\n    valueFrom:\n      secretKeyRef:\n        name: thehyve\n        key: your_secret_key\n","language":"yaml","time":"15 days ago","created":"2025-10-25T11:40:27.537Z","annotations":[],"updated":"2025-10-25T11:40:30.283Z"},{"title":"Extra Deploy for Vault Static Secret","id":"d7ee896d-fdd4-45ad-bfdc-6ab3ced51057","type":"CODE","raw":"extraDeploy:\n  - apiVersion: secrets.hashicorp.com/v1beta1\n    kind: VaultStaticSecret\n    metadata:\n      name: thehyve\n      namespace: \"{{ .Release.Namespace }}\"\n    spec:\n      namespace: admin/deployments/ff-c # ‚Üê Vault namespace\n      mount: secrets\n      path: thehyve # ‚Üê Secret path in Vault\n      type: kv-v2\n      hmacSecretData: true\n      destination:\n        create: true\n        name: thehyve\n        transformation:\n          excludes:\n            - .*\n          templates:\n            qcr_bucket:\n              text: '{{`{{get .Secrets \"qcr_bucket\"}}`}}'\n            qcr_access_key_id:\n              text: '{{`{{get .Secrets \"qcr_access_key_id\"}}`}}'\n            qcr_secret_access_key:\n              text: '{{`{{get .Secrets \"qcr_secret_access_key\"}}`}}'\n            qcr_iam_role:\n              text: '{{`{{get .Secrets \"qcr_iam_role\"}}`}}'\n            # Add your new secret template here:\n            your_secret_key:\n              text: '{{`{{get .Secrets \"your_secret_key\"}}`}}'\n","language":"yaml","time":"15 days ago","created":"2025-10-25T11:40:32.060Z","annotations":[],"updated":"2025-11-03T13:15:22.790Z"},{"title":"AWS sso login discovery access.","id":"7705508e-ddad-400f-8554-8dbadcbc5ac3","type":"CODE","raw":"aws sso login --profile discovery-access\n","language":"sh","time":"8 days ago","created":"2025-11-01T15:10:19.920Z","annotations":[],"updated":"2025-11-03T13:14:48.569Z"},{"title":"Remove 0.0.0/0 rules and replace with VPC CIDR","id":"648ab39b-87de-4c78-b1d4-ef36f8751d17","type":"CODE","raw":"# Remove the 0.0.0.0/0 rules and replace with VPC CIDR\naws ec2 revoke-security-group-ingress \\\n  --group-id sg-02dcb1a5bbe8844b8 \\\n  --ip-permissions IpProtocol=tcp,FromPort=31139,ToPort=31139,IpRanges='[{CidrIp=0.0.0.0/0}]'\n\naws ec2 revoke-security-group-ingress \\\n  --group-id sg-02dcb1a5bbe8844b8 \\\n  --ip-permissions IpProtocol=tcp,FromPort=32623,ToPort=32623,IpRanges='[{CidrIp=0.0.0.0/0}]'\n","language":"sh","time":"6 days ago","created":"2025-11-03T13:12:45.159Z","annotations":[],"updated":"2025-11-03T13:13:00.173Z"},{"title":"Check if image version is provided","id":"99dad9fc-31b6-4f19-ad3b-da67a2e1f9fe","type":"CODE","raw":"#!/bin/bash\n\n# Check if an image version is provided\nif [ -z \"$1\" ]; then\n  echo \"Usage: $0 <image_version>\"","language":"sh","time":"a day ago","created":"2025-11-08T12:02:58.880Z","annotations":[],"updated":"2025-11-08T12:03:00.871Z"},{"title":"Image Version Validation and GitHub Authentication","id":"918e1967-6c60-421e-8806-0c9852659127","type":"CODE","raw":"#!/bin/bash\n\n# Check if an image version is provided\nif [ -z \"$1\" ]; then\n  echo \"Usage: $0 <image_version>\"\n  echo \"Example: $0 0.4.4-test\"\n  exit 1\nfi\n\nIMAGE_VERSION=\"$1\"\nGHCR_REGISTRY=\"ghcr.io\"\nGHCR_REPO=\"thehyve/fitfile_etl_container_mkuh\"\nACR_REGISTRY_NAME=\"fitfileregistry\"\nACR_LOGIN_SERVER=\"${ACR_REGISTRY_NAME}.azurecr.io\"\nACR_REPO=\"thehyve/fitfile_etl_container_mkuh\" # Assuming same repo name in ACR as GHCR\n\nGHCR_FULL_IMAGE=\"${GHCR_REGISTRY}/${GHCR_REPO}:${IMAGE_VERSION}\"\nACR_FULL_IMAGE=\"${ACR_LOGIN_SERVER}/${ACR_REPO}:${IMAGE_VERSION}\"\n\necho \"--- Authenticating to GitHub Container Registry (ghcr.io) ---\"\n# Refresh GitHub CLI Token with read:packages Scope\ngh auth refresh -h github.com -s read:packages || { echo \"Failed to refresh GitHub token.\"; exit 1; }\n\n# Use the Token to Authenticate Docker to ghcr.io\ngh auth token | docker login \"${GHCR_REGISTRY}\" -u leonormes --password-stdin || { echo \"Failed to login to GHCR.\"; exit 1; }\necho \"Successfully logged into ghcr.io\"\n\necho \"--- Pulling image from GitHub Container Registry ---\"\ndocker pull \"${GHCR_FULL_IMAGE}\" || { echo \"Failed to pull image from GHCR: ${GHCR_FULL_IMAGE}\"; exit 1; }\necho \"Successfully pulled ${GHCR_FULL_IMAGE}\"\n\necho \"--- Authenticating to Azure Container Registry (${ACR_REGISTRY_NAME}) ---\"\naz acr login --name \"${ACR_REGISTRY_NAME}\" || { echo \"Failed to login to Azure Container Registry.\"; exit 1; }\necho \"Successfully logged into ${ACR_REGISTRY_NAME}\"\n\necho \"--- Tagging image for Azure Container Registry ---\"\ndocker tag \"${GHCR_FULL_IMAGE}\" \"${ACR_FULL_IMAGE}\" || { echo \"Failed to tag image.\"; exit 1; }\necho \"Successfully tagged ${GHCR_FULL_IMAGE} as ${ACR_FULL_IMAGE}\"\n\necho \"--- Pushing image to Azure Container Registry ---\"\ndocker push \"${ACR_FULL_IMAGE}\" || { echo \"Failed to push image to ACR: ${ACR_FULL_IMAGE}\"; exit 1; }\necho \"Successfully pushed ${ACR_FULL_IMAGE} to ${ACR_REGISTRY_NAME}\"\n\necho \"Script completed successfully.\"","language":"sh","time":"a day ago","created":"2025-11-08T12:12:03.642Z","annotations":[],"updated":"2025-11-08T12:12:28.147Z"}],"gptContexts":[],"gptFolderNames":[],"enrichedCode":{}}