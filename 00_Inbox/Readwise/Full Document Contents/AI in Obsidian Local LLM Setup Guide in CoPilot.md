# AI in Obsidian: Local LLM Setup Guide in CoPilot

![rw-book-cover](https://i.ytimg.com/vi/hOaSO_e7MYs/maxresdefault.jpg)

## Metadata
- Author: [[Paul Dickson]]
- Full Title: AI in Obsidian: Local LLM Setup Guide in CoPilot
- Category: #articles
- Summary: This guide explains how to use AI inside Obsidian with the CoPilot plugin and LM Studio. You can load and run local AI models or connect to OpenAI using API keys. The setup lets you chat with AI, save notes, and search your Obsidian vault using AI tools.
- URL: https://www.youtube.com/watch?v=hOaSO_e7MYs

## Full Document
Hi, I'm Paul, and in today's video, we'llintegrate local and cloud AI tools inObsidian using LM Studio and theCo-Pilot plugin. This setup will let youuse AI directly within Obsidian tosummarize notes, find connections, andboost your productivity. I'll walk youthrough my setup, then show you how tocreate your own AI-powered ObsidianVault. Join me as we power up yourObsidian Vault with AI.

Introducing artificial intelligence in Obsidian: theCo-Pilot plugin for Obsidian is an opensource LLM interface right insideObsidian. It has a minimalistic designand is straightforward to use. LM Studiois a desktop app for developing andexperimenting with LLMs locally on yourcomputer. Deep Seek is a Chineseartificial intelligence company thatdevelops and releases large languagemodels. The most well-known is theopen-source model Deep Seek R1671B, which can compete with OpenAI andGoogle Bard for a fraction of the cost.

Open Web UI is an open-sourceself-hosted AI interface that provides auser-friendly way to interact withvarious large language models. It can berun entirely offline, giving you moreControl over your data and privacy. TheObsidian custom frames plugin turns webapps into panes using iframes withcustom styling. I used this to load my openweb UI AI tools inside of Obsidian. AItools inside Obsidian showcase. If I wantto use AI tools inside of Obsidian, thenI have a couple of options. The first oneis to use a commander shortcut to thecustom frames plugin. I’ve set up acustom frame which goes straight to myAI tool and I can select which model ofAI I would like to use right hereinside of Obsidian. The other option isto use another community plugin calledCo-Pilot for Obsidian, and inside here Isimply have a few models to choose from.

So the best thing about this plugin isyou can add your own custom models, andyou can do this with any of theproviders listed here. One of thoseproviders is LM Studio. So you downloadyour open-source large language model.Then you select your model, load yourmodel. You have your model loaded. I findthe smaller models perform a lot betterand are less resource-intensive. Then youcan either chat directly inside LMStudio on a second monitor whilst you’reworking inside of Obsidian, or you couldUse your open web UI tools on a secondmonitor. If you wanted to bring thoseinto Obsidian with the Co-Pilot plugin,then you can do that with your API key,depending on what AI tool you're using.

The cost is going to be different foreach one. To access Co-Pilot, I simplyhit Control P, type in the word Co-Pilot,and it gives me a whole bunch ofoptions. I can open the Co-Pilot chatwindow. It opens up the Co-Pilot pluginin the right pane, similar to customframes. Then I can just simply choose mymodel here and then start entering in myrequests. The benefit of doing this is Ican save my chats as a note. Co-Pilot hasthe ability to interact with your entireVault using Vault QA mode, so you simplyjust change the chat to Vault QA mode.This will use an embedded model whichis locally indexed to ensure your dataisn't sent to the cloud. You'll see a fewsuggested prompts in the chat window. Youcan also ask Co-Pilot about your note, sofor this note that's open, I can askCo-Pilot to summarize thenote. Then, if I drag that open, I can seemy response in the chat window. Co-Pilot

also allows me to find relevant notes, soyou can see up at the top here, I've got aA little drop-down arrow, then I can seethat there are some similar notes to thisnote that I have open, and I can see thescore by rolling over it. I could add itto the chat and ask aquestion. Then I can go through thesimilarities of these twobooks. Up until this point, I've used AItools on a second monitor through anopen web UI, which makes externalrequests to my AI providers. I pay amonthly subscription in order to usethis service, and I find it works wellfor me. Sometimes, I do use the customframes plug-in inside of Obsidian tobring that open web UI into Obsidian inorder to use it on the right pane. I'veonly just really started experimentingwith the Co-Pilot plugin and hostinglarge language models locally on LM.

I'm finding running the largermodels is very resource-intensive, andI'm not too comfortable running the 7Band above models. It's a little bit tooslow for me and resource-intensive, butfor interacting with my notes, I'mexperimenting with the Co-Pilot plugin.That's what's inspired me to makethis video for you guys today: how to setup a local large language model.

LM Studio will allow us to discover,download, and run local large languagemodels on our computer. With LM Studio, wecan run a large language model entirelyoffline and chat with our local documentsright in the privacy of our own home. I’mrunning a 13700K with an RTX 480, whichhas 16 GB of VRAM. To get started insideLM's GD, I need to navigate over to theDiscover tab on the left-hand side. Thispresents me with a search field, and Ineed to enter the name of the largelanguage model that I would like todownload. I’m going to download theslightly controversial Deep Seek R1,which is rivaling OpenAI's Model 01. So I

just simply type in Deep Seek R1. That’sgoing to show me some results. So we cansee here I’ve got Deep Seek R1 7B, and Ican go down to the bottom right anddownload this model. It’s 4.68 GB. I’mgoing to go ahead and do that. It willpop up with the download window, andthat’s going to give me an indication ofhow much time it’s going to take todownload this model. You can see downhere I’ve already pre-downloaded a fewother models. So once your downloadcompletes, it will show up under the MyModels directory.

I've got 1.5B, 3.23B, 7B, 14B, and 8B, so my computer canhandle up to 14B. The "B" stands for billion parameters. You can see the sizeover to the right here for this video.I'll just go ahead and run the 1.5Bmodel because this is going to ensurethat I have really good performanceinside of Obsidian. So if you're runningsomething locally, just be mindful thatif you do run the 7B model, it's going touse a lot of your CPU, GPU, and RAM andthat may slow down your computer. So ifyou want to host this on a secondcomputer, then I'd recommend using thelarger models. But if you just wantsomething quick and local, private insideObsidian, then the 1.5B model or Llama 3Bis going to be the best you can choose.Your model directory is up the top here, andyou can change that to whatever youwould like. I've put mine on a D Drive ina particular folder. Then if I go into mydeveloper tab inside here, I have alittle option up the top to select amodel to load. So go ahead and select my1.5B model here. It's going to ask mewhat the context length of this model is.

My GPU offload is how muchPower, I'm going to allow this model touse from the GPU. I'll dial it down a fewnotches so it’s not using the whole GPU,but you can play around with this. ThenI'll leave the CPU threads at 9 becauseI don’t want it to be using the full CPUeither. Then everything else I'lljust leave as default. You can checkthe remember settings for this model andthen you just hit load model. So that’sgoing to go ahead and load thatparticular model. Now I also need to loadan embed for Obsidian to index my notesso that I can query the AI about thoseparticular notes. So I’ll go ahead andselect that one. I’m just going to leaveit default because it’s quite a smallmodel. Then it’s going to go ahead andload that model. We’ll then come up tothe Things tab and just check yourserver port number, enable cross-originresource sharing, then go ahead and startyour server. If everything works as itshould, you’ll see in the developer logshere that it’s listing on the port thatyou’ve specified. By default, it should be 1,2, 3, 4, and then I can see it’s running onLocalhost. So this justessentially means that these two modelsare ready to go. I could use this to chat.

Inside of LM Studio on a second monitor,alongside Obsidian, the benefit of thisis I can add images here if I want to,and it runs a little bit quicker, so youcan see it thought for six seconds on that.But if I want to use it inside Obsidian,I jump back into Obsidian and open up myCo-Pilot and select my 1.5 Bravo model.I go grab our message here: what is thecapital of France? Then I paste that in. Youcan see it's still quite responsive, butwe don’t have to step outside ofObsidian. So that’s taking nine seconds,which is three seconds longer to use theplug-in inside of Obsidian compared togoing direct to LM Studio.

How to set up AI tools in Obsidian: So if we click onthe cogwheel and navigate to Communityplugins, select browse, and type in theword Co-Pilot. We go ahead and installand enable the Co-Pilot plugin. Once I'veenabled it, I’ll jump into the options. Ifyou already have an OpenAI API key, then youcan set that key inside the API Keyshere. If you've purchased a Co-Pilot Pluslicense, then you can enter it here. I'mgoing to head over to the model tab, andinside here, I can see the default loadedmodels. So I'm going to just remove allof these, and I'm going to uncheck theEnabled default models here. I'm going tokeep everything else default here, andI'm going to go up to add a custom model.

The first one I want to add is anexternal open web UI API that I have onthe right-hand side here for my CLA 3.5Sonet. So I'm just simply going to navigatedown to OpenAI format, then I'mgoing to put in my base URL, and theseare where my OpenAI tools are hosted.I then need to enter my API key, soI'll place my API key in where it saysAPI key. I need to specify which model Iwant to bring into Copilot, so I'm justgoing to use GPT-4. You can see downhere I can add or verify, so I'll selectverify. That's going to do a model

verification up the top, so I'll add thatmodel, and now I have my custommodel added, GPT-4. I’ll do the samefor Claude 3.5 Sonet, so I’ll paste inthe model name. Then I’ll simply justrepeat the same steps as I did before byputting in my base URL and my API key. Ican hit verify again, and we can see themodel for verification is successful. Sonow I've got OpenAI and Entropic AItools added. I could also add Deep SeekR1, which is making headlines recently. SoI'll go put my OpenAI format now.

One is posted in the US, so it's notgoing to China, and that'sjust going to ensure that I maintainsome good privacy measures using thismodel that one has verified as well.So now I have my three models, and I canadd as many as I want. I'll just addthree to save time. So now I have my chatmodels enabled. I need to go down and addmy embedding models. So you can see herethere are a few that come included, so Ican either remove those or just leavethem as they are. I'm going to add acustom model. Then it's going to ask mefor the name of my model. I'll bring upLM Studio, and I've got some local largelanguage models running in LM Studio.

I need an embedding model, so I'mgoing to use this one here, textembedding nomic embed 1.5. So I'll simplycopy this one, paste that in as my modelname. Now I'm running this locally on mylocal server. You can see it's running upthe top here. Then I'm going to come downand change my provider to LM Studio, so Isimply copy that, paste that in as mymodel name. Now I'm going to grab my local serveraddress which is on my local host onPort 1235 and paste that in.

To put a for SLV1, I'll hit verify onthat we can see it's verified the modelup the top there. So I'll add that model,and now I have my embedding model added.I come back to my basic tab, and Iwant to change my default chat model toone of the three models that I've added.So I'm going to set it to Clawas my default. If you want a bit morespeed, then you can go for a faster model.Make sure your embedding model is yourlocal one stored in LM Studio. It's goingto ask us to index our vault, and don'tworry, it does this locally, so it’s notgoing to send your data out to OpenAI.So we'll hit continue. You'll notice atthe top, CoPilot will start indexing yourvault, so we'll just let that run whilstwe continue. Our default mode will bechat, but we can change that if we want.

We'll open the plugin in the sidebar.We can change any commands in here forour model if we don't want to use them.I'll just leave them as default for now.The only other thing I usually do isturn on encryption for our API keys, andthen everything else can be left as is.So now we just need to have a cup ofcoffee and wait for our CoPilot toindex our vault. I'll just bring up LMStudio, while we're waiting, you cansee what it's doing with my notes in thedeveloper logs here. You can see it'sindexed all my notes. Co-pilot will placea shortcut on the ribbon, so we'll selectour open co-pilot chart that’s going toreplace my custom iframe with theco-pilot. You can see down the bottomhere, I've got my little chat window, andthen I've got chat selected with thename of my model. Here, we'll start withGPT-4. I'm just going to prompt for aquestion, so you can see that itresponded quite quickly. It's given me ananswer. I can move this out to get alittle bit more information.

I could also save this chat as a note by clickingthis button here, or I've got the threedots where I can select to suggest theprompt, relevant notes, or refresh theindex. I can delete my question here, editor copy it. If I want to take thistext and copy it, I can right-click andcopy. If I want to start a new chat, thenI simply just select new chat here, andthat’s going to start a new chat. I canask another question, and it’s respondedquite quickly. So let's try another model.We’ll try CLA 3.5. So that one’salso responded quite quickly.

Try our embed model so we can switchover to our vault.You'll see ourvault index is up to date and you'll seethis little prompt here: "Ask anything fornotes with custom prompts." So if we justbring up this note 'Mindset,' and I'm justgoing to say, "Can yousummarize my note and put double squarebrackets?" It's going to prompt us for thecurrent note, so you can see it'sthinking about it, and then it gives me aresponse. I can either save this chat asa note or I can copy it into thisexisting note. I want to have a look atrelevant notes. I've got some links uphere. If I click the drop-down, it'll showme all the relevant notes. So we've got

one here, "As a Man Thinketh" by JamesAllen. So I can go select that one. Icould select "Be Unstoppable" by AdenMills, which is another relevant note. Sothe AI is suggesting relevant notes forme. If it's taking too much of yourObsidian vault, then you can hide and goback to your chat. Just make sure youchange it back to chat. Now, if youweren't comfortable using an external APIrequest because you're not sure how manytokens you're going to be using, andyou're cognizant of how much it's goingTo cost, then you can host these locallyin LM Studio. So if I jump into LM Studio,I have a few models that I've downloaded.

I've been playing around with all theDeep Seek R1 models and Llama 3.2. I'mrunning an RTX 480, so I can run models upto 14 billion parameters, but it's prettyresource-intensive and can be quite slow.So I still prefer using my OpenAI APItools because they're quicker, but if Iwanted to run something locally and haveit privately running, then I'd probablyload something like a 1.5 billionparameter model. It's not going to be assmart, but it allows me to keep my Vaultprivate. So I'll show you a case of how to dothat. So we'll jump back into Obsidian.

We'll go into our Co-Pilot again. We'llhead over to models, and we're going toadd a custom model again, and we'llchange this one to LM Studio. Then I'mgoing to go grab my model here,which is the 1.5 billion parameter model,and I'll paste that in as my model name.I'll grab my base URL, which is my localhost on port1235. Then I'm just going to hit verify,and because I haven't put theSLV1, I need to hit verify again, andnow my model is successful.

To add Deep Seek R1, I now have DeepSeek R1 1.5B added to CoPilot.I can come down here and choose to use that localmodel. I'm going to ask a similarquestion now. If we bring up LM Studio atthe same time, we can see that it'sgenerating a prompt that I've just madeinside of Obsidian, and it's gone aheadand responded within 9 seconds from thatDeep Seek R1 1.5B model. You can seeit's given me a little bit moreinformation than just saying the capitalof France is Paris.It's not been too resource heavy, soI'm not noticing too much of a performance issueinside of Obsidian. It's also private, soI'm not going out to the internet to getmy response. If this is something thatmight interest you, I recommendexperimenting with these AI tools inyour Obsidian vault. So, there you have it:powerful AI integrated right into yourObsidian vault.

Let me know in the comments how you're using AI withObsidian, from summarizing to ideageneration. I'd love to hear your ideas.If you found it helpful, please like andsubscribe for more Obsidian tips andtricks. Thanks for watching, and I'll see youSee you in the next video.

[Music]
