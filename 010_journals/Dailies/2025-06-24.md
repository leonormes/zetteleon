---
aliases: []
confidence: 
created: 2025-06-24T08:38:11Z
epistemic: 
journal: Daily
journal-date: 2025-06-24
last_reviewed: 
modified: 2025-11-03T13:48:40Z
purpose: 
review_interval: 
see_also: []
source_of_truth: []
status: 
tags: []
title: 2025-06-24
type:
uid: 
updated: 
version:
---

```journal-nav

```

EoE node needs to call the FITConnect APIs of the CUH node. FITFILE will deploy an internal load balancer into our VPC. We can specify the IP of this load balancer, and it will need to be an available IP from the system subnet.

So whoever is responsible for the networking needs to poke a hole in the firewall to allow inbound HTTPS traffic on 443 to figure out the routing between the on-premise and the shared-services hub network (which will be peered to our VPC).

In addition, a bunny node will be deployed into the cluster. This needs to call out over port 443 to the EoE Relay API (Leon Ormes knows the address for this). So we need the reverse networking and outbound rule for this.

Another issue is how are they going to access the web app? Are they using VDIs? Does it need to be accessible in their on-premise network?

- They have an always on VPN for all devices - plan is to have a forward lookup zone to the azure private DNS
- The ip ranges were causing problems. The max surge was 100 so AKS wanted more IP. We are using Calico but it is not deployed yet? How can we set this up so Calico IP management is used at this stage?

```sh
kubectl get nodes -o wide
NAME                                STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
aks-system-19042853-vmss000000      Ready    <none>   14m     v1.31.9   10.250.16.5    <none>        Ubuntu 22.04.5 LTS   5.15.0-1090-azure   containerd://1.7.27-1
aks-system-19042853-vmss000001      Ready    <none>   14m     v1.31.9   10.250.16.6    <none>        Ubuntu 22.04.5 LTS   5.15.0-1090-azure   containerd://1.7.27-1
aks-workflows-32479518-vmss000000   Ready    <none>   9m22s   v1.31.9   10.250.16.20   <none>        Ubuntu 22.04.5 LTS   5.15.0-1090-azure   containerd://1.7.27-1

kubectl get pods --all-namespaces -o wide
NAMESPACE           NAME                                            READY   STATUS    RESTARTS      AGE   IP             NODE                                NOMINATED NODE   READINESS GATES
calico-system       calico-kube-controllers-6bdfcdbcb5-2x2mf        1/1     Running   0             12m   10.244.0.122   aks-system-19042853-vmss000001      <none>           <none>
calico-system       calico-node-5jmtn                               1/1     Running   0             12m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
calico-system       calico-node-cnz5f                               1/1     Running   0             10m   10.250.16.20   aks-workflows-32479518-vmss000000   <none>           <none>
calico-system       calico-node-kpdtf                               1/1     Running   0             12m   10.250.16.6    aks-system-19042853-vmss000001      <none>           <none>
calico-system       calico-typha-6cd56d6b8f-b4qxx                   1/1     Running   0             10m   10.250.16.6    aks-system-19042853-vmss000001      <none>           <none>
calico-system       calico-typha-6cd56d6b8f-d8qv9                   1/1     Running   0             12m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
gatekeeper-system   gatekeeper-audit-6c74dbb7b6-4g2w6               1/1     Running   0             17m   10.244.0.201   aks-system-19042853-vmss000001      <none>           <none>
gatekeeper-system   gatekeeper-controller-6d4cc8855-fcwgg           1/1     Running   0             17m   10.244.0.176   aks-system-19042853-vmss000001      <none>           <none>
gatekeeper-system   gatekeeper-controller-6d4cc8855-qhdgq           1/1     Running   0             17m   10.244.0.33    aks-system-19042853-vmss000001      <none>           <none>
kube-system         azure-cns-hcr6m                                 1/1     Running   0             15m   10.250.16.6    aks-system-19042853-vmss000001      <none>           <none>
kube-system         azure-cns-pxzgb                                 1/1     Running   0             15m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
kube-system         azure-cns-r664g                                 1/1     Running   0             10m   10.250.16.20   aks-workflows-32479518-vmss000000   <none>           <none>
kube-system         azure-ip-masq-agent-sqcp9                       1/1     Running   0             10m   10.250.16.20   aks-workflows-32479518-vmss000000   <none>           <none>
kube-system         azure-ip-masq-agent-vhkbl                       1/1     Running   0             15m   10.250.16.6    aks-system-19042853-vmss000001      <none>           <none>
kube-system         azure-ip-masq-agent-vhzkx                       1/1     Running   0             15m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
kube-system         azure-policy-786dfcc5c6-7z4r4                   1/1     Running   0             17m   10.244.0.137   aks-system-19042853-vmss000001      <none>           <none>
kube-system         azure-policy-webhook-cd69cd6bf-6wrwz            1/1     Running   1 (14m ago)   17m   10.244.0.165   aks-system-19042853-vmss000001      <none>           <none>
kube-system         cloud-node-manager-2997r                        1/1     Running   0             15m   10.250.16.6    aks-system-19042853-vmss000001      <none>           <none>
kube-system         cloud-node-manager-qjj5p                        1/1     Running   0             10m   10.250.16.20   aks-workflows-32479518-vmss000000   <none>           <none>
kube-system         cloud-node-manager-svdqr                        1/1     Running   0             15m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
kube-system         coredns-789465848c-5nthq                        1/1     Running   0             17m   10.244.0.91    aks-system-19042853-vmss000001      <none>           <none>
kube-system         coredns-789465848c-m8qgg                        1/1     Running   0             14m   10.244.1.112   aks-system-19042853-vmss000000      <none>           <none>
kube-system         coredns-autoscaler-55bcd876cc-t2sj5             1/1     Running   0             17m   10.244.0.42    aks-system-19042853-vmss000001      <none>           <none>
kube-system         csi-azuredisk-node-5ls4l                        3/3     Running   0             15m   10.250.16.6    aks-system-19042853-vmss000001      <none>           <none>
kube-system         csi-azuredisk-node-9s7m4                        3/3     Running   0             10m   10.250.16.20   aks-workflows-32479518-vmss000000   <none>           <none>
kube-system         csi-azuredisk-node-t9csw                        3/3     Running   0             15m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
kube-system         csi-azurefile-node-dfd9q                        3/3     Running   0             15m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
kube-system         csi-azurefile-node-jvj4j                        3/3     Running   0             10m   10.250.16.20   aks-workflows-32479518-vmss000000   <none>           <none>
kube-system         csi-azurefile-node-mbf54                        3/3     Running   0             15m   10.250.16.6    aks-system-19042853-vmss000001      <none>           <none>
kube-system         konnectivity-agent-77ccb94b69-kcfrr             1/1     Running   0             13m   10.244.0.234   aks-system-19042853-vmss000001      <none>           <none>
kube-system         konnectivity-agent-77ccb94b69-r8s7m             1/1     Running   0             13m   10.244.1.45    aks-system-19042853-vmss000000      <none>           <none>
kube-system         konnectivity-agent-autoscaler-679b77b4f-x2qh5   1/1     Running   0             17m   10.244.0.76    aks-system-19042853-vmss000001      <none>           <none>
kube-system         kube-proxy-j2t87                                1/1     Running   0             15m   10.250.16.6    aks-system-19042853-vmss000001      <none>           <none>
kube-system         kube-proxy-m5kbs                                1/1     Running   0             15m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
kube-system         kube-proxy-p2xp5                                1/1     Running   0             10m   10.250.16.20   aks-workflows-32479518-vmss000000   <none>           <none>
kube-system         metrics-server-85cf858c74-6c54d                 2/2     Running   0             14m   10.244.1.32    aks-system-19042853-vmss000000      <none>           <none>
kube-system         metrics-server-85cf858c74-vr22p                 2/2     Running   0             14m   10.244.1.250   aks-system-19042853-vmss000000      <none>           <none>
kube-system         vpa-admission-controller-649c99d56f-6bmgx       1/1     Running   0             12m   10.244.0.223   aks-system-19042853-vmss000001      <none>           <none>
kube-system         vpa-admission-controller-649c99d56f-s4qmt       1/1     Running   0             12m   10.244.1.228   aks-system-19042853-vmss000000      <none>           <none>
kube-system         vpa-recommender-6545476785-v8zzk                1/1     Running   0             12m   10.244.0.236   aks-system-19042853-vmss000001      <none>           <none>
kube-system         vpa-updater-76c8ffcd4d-mgdjh                    1/1     Running   0             12m   10.244.1.188   aks-system-19042853-vmss000000      <none>           <none>
tigera-operator     tigera-operator-5c9f889dbc-rpv42                1/1     Running   0             13m   10.250.16.5    aks-system-19042853-vmss000000      <none>           <none>
```

## Analysis of Your Output

**1. Node IPs are Correct**

Your `kubectl get nodes` output shows:

- `aks-system-*`nodes have IPs like `10.250.16.5` and `10.250.16.6`.
- `aks-workflows-*` node has an IP of `10.250.16.20`.

These IPs fall perfectly within the `/28` subnets we defined in locals.tf. This confirms the nodes are correctly placed within your VNet.

**2. Pod IPs are Correct (and this is the key!)**

Your `kubectl get pods --all-namespaces` output is the most important piece of evidence.

- **Overlay Pods:** Look at the IPs for pods like `coredns`, `gatekeeper-controller`, and `konnectivity-agent`. They have IPs like `10.244.0.91`, `10.244.1.112`, and `10.244.0.176`. These are all from the `10.244.0.0/16` overlay CIDR we configured. This proves that pod IPs are not being taken from your VNet.
- **Host Network Pods:** You'll notice some pods, particularly `calico-node-*`, `azure-cns-*`, and `kube-proxy-*`, have IPs that match the node they are running on (e.g., `10.250.16.5`). This is also expected and correct. These are critical system pods that run as DaemonSets with `hostNetwork: true`, meaning they intentionally use the node's own network interface instead of getting an IP from the overlay.

**3. Calico Pods are Running (Just in a Different Namespace)**

You were very close with your command to check for Calico pods. The reason it returned nothing is that the Calico components live in their own `calico-system` namespace, not  `kube-system`

You can confirm they are running with this corrected command:

kubectl get pods -n calico-system

You'll see the `calico-node`and `calico-kube-controllers`

```sh
get pods --all-namespaces
```

## Conclusion

The evidence is clear:

- **Nodes** live in your VNet (

  ```sh
  10.250.16.0/24
  ```

  ).

- **Pods** live in the overlay network (

  ```sh
  10.244.0.0/16
  ```

  ).

This is the exact outcome we aimed for. You have successfully deployed an AKS cluster using the Azure CNI Overlay with Calico, which gives you a massive, scalable IP space for your applications without consuming your limited VNet addresses.

```sh
kubectl get pods -n calico-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6bdfcdbcb5-2x2mf   1/1     Running   0          14m
calico-node-5jmtn                          1/1     Running   0          14m
calico-node-cnz5f                          1/1     Running   0          12m
calico-node-kpdtf                          1/1     Running   0          14m
calico-typha-6cd56d6b8f-b4qxx              1/1     Running   0          12m
calico-typha-6cd56d6b8f-d8qv9              1/1     Running   0          14m
```

Seeing the

```sh
calico-node
```

,

```sh
calico-kube-controllers
```

, and

```sh
calico-typha
```

 pods all in a

```sh
Running
```

 state in the

```sh
calico-system
```

 namespace confirms that the Calico CNI and network policy engine are fully operational.

This completes our validation. Every piece of evidence points to a successful deployment that perfectly matches the architecture we designed:

- **Nodes** are in your VNet.
- **Pods** are in the CNI overlay network.
- **Calico** is running and managing the network.

You've successfully navigated a complex infrastructure refactoring, from fixing subnet conflicts to enhancing your Terraform module for a more advanced, scalable, and robust networking model. Congratulations on a job well done!
