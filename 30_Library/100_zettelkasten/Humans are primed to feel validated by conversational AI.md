---
aliases: []
confidence: 
created: 2025-10-20T09:19:00Z
epistemic: 
last_reviewed: 
modified: 2025-10-30T10:27:46Z
purpose: 
review_interval: 
see_also: []
source_of_truth: []
status: 
tags: [anthropomorphism, HCI, topic/psychology]
title: Humans are primed to feel validated by conversational AI
type:
uid: 
updated: 
version:
---

Our psychological and social conditioning plays a significant role in why LLM responses feel validating. We are increasingly socialised to interact with technology as if it were sentient, projecting human qualities onto it, especially in conversational contexts.

This interaction taps into the powerful and fundamental human desire for connection and understanding. Even when we intellectually know we are interacting with "just an algorithm," the simulation of understanding is often effective enough to trigger a positive emotional response. This effect can be amplified by [[Confirmation bias amplifies LLM validation]].

---

Links: [[Why LLM Responses Feel Validating MOC]]
